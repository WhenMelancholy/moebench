{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a0ebd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63ea0768abc45b8abe3d9af46628273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "os.chdir(\"/n/home08/zkong/mufan/tmp/moebench/open-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"output/deepseek-moe-16b-chat\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01bdf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepseekForCausalLM(\n",
       "  (model): DeepseekModel(\n",
       "    (embed_tokens): Embedding(102400, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0): DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (down_proj): Linear(in_features=10944, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "      (1-27): 27 x DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMoE(\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x DeepseekMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (gate): MoEGate()\n",
       "          (shared_experts): DeepseekMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (down_proj): Linear(in_features=2816, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): DeepseekRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14d4da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m            DeepseekForCausalLM\n",
      "\u001b[0;31mString form:\u001b[0m    \n",
      "DeepseekForCausalLM(\n",
      "           (model): DeepseekModel(\n",
      "           (embed_tokens): Embedding(102400, 2048)\n",
      "           (l <...> ): DeepseekRMSNorm()\n",
      "           )\n",
      "           (lm_head): Linear(in_features=2048, out_features=102400, bias=False)\n",
      "           )\n",
      "\u001b[0;31mFile:\u001b[0m            /n/netscratch/mzitnik_lab/Lab/zlkong/cache/modules/transformers_modules/deepseek-moe-16b-chat/modeling_deepseek.py\n",
      "\u001b[0;31mDocstring:\u001b[0m       <no docstring>\n",
      "\u001b[0;31mClass docstring:\u001b[0m\n",
      "The bare Deepseek Model outputting raw hidden-states without any specific head on top.\n",
      "This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
      "library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
      "etc.)\n",
      "\n",
      "This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
      "Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
      "and behavior.\n",
      "\n",
      "Parameters:\n",
      "    config ([`DeepseekConfig`]):\n",
      "        Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
      "        load the weights associated with the model, only the configuration. Check out the\n",
      "        [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
      "\u001b[0;31mInit docstring:\u001b[0m  Initialize internal Module state, shared by both nn.Module and ScriptModule."
     ]
    }
   ],
   "source": [
    "?model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
