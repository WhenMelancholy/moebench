{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "\n",
    "os.chdir(\"/n/home08/zkong/mufan/tmp/moebench/open-instruct\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "model_dir = \"output/0304_lima_expert\"\n",
    "result_dir = f\"results/mode/.__output__{os.path.basename(model_dir)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arc_challenge': 0.4641638225255973, 'arc_easy': 0.7878787878787878, 'mmlu': 0.5089018658310782, 'piqa': 0.8046789989118607, 'winogrande': 0.6882399368587214, 'nq': 0.1368421052631579, 'truthfulqa_mc1': 0.23378212974296206, 'truthfulqa_mc2': 0.35617605142565717}\n",
      "{'arc_challenge': 0.19368600682593856, 'arc_easy': 0.27230639730639733, 'mmlu': 0.24405355362483977, 'piqa': 0.5125136017410229, 'winogrande': 0.47750591949486976}\n",
      "{'arc_challenge': 0.43600682593856654, 'arc_easy': 0.7550505050505051, 'mmlu': 0.4188861985472155, 'piqa': 0.7878128400435256, 'winogrande': 0.6393054459352802}\n",
      "dict_values([0.4641638225255973, 0.7878787878787878, 0.5089018658310782, 0.8046789989118607, 0.6882399368587214, 0.1368421052631579, 0.23378212974296206, 0.35617605142565717])\n",
      "dict_values([0.19368600682593856, 0.27230639730639733, 0.24405355362483977, 0.5125136017410229, 0.47750591949486976])\n",
      "dict_values([0.43600682593856654, 0.7550505050505051, 0.4188861985472155, 0.7878128400435256, 0.6393054459352802])\n"
     ]
    }
   ],
   "source": [
    "def find_latest_json_file(directory):\n",
    "    # 获取目录下所有以 .json 结尾的文件列表\n",
    "    json_files = [f for f in os.listdir(directory) if f.endswith(\".json\")]\n",
    "\n",
    "    # 如果没有找到任何 .json 文件，返回 None\n",
    "    if not json_files:\n",
    "        return None\n",
    "\n",
    "    # 获取每个文件的完整路径\n",
    "    full_paths = [os.path.join(directory, f) for f in json_files]\n",
    "\n",
    "    # 找到最新的文件\n",
    "    latest_file = max(full_paths, key=os.path.getmtime)\n",
    "\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "def get_acc(mode):\n",
    "    result_path = find_latest_json_file(result_dir.replace(\"mode\", mode))\n",
    "    result = json.load(open(result_path))\n",
    "\n",
    "    datasets = [\n",
    "        \"arc_challenge\",\n",
    "        \"arc_easy\",\n",
    "        \"mmlu\",\n",
    "        \"piqa\",\n",
    "        \"winogrande\",\n",
    "    ]\n",
    "    acc = {dataset: result[\"results\"][dataset][\"acc,none\"] for dataset in datasets}\n",
    "\n",
    "    if mode == \"baseline\":\n",
    "        acc[\"nq\"] = result[\"results\"][\"nq_open\"][\"exact_match,remove_whitespace\"]\n",
    "        datasets = [\"truthfulqa_mc1\", \"truthfulqa_mc2\"]\n",
    "        for dataset in datasets:\n",
    "            acc[dataset] = result[\"results\"][dataset][\"acc,none\"]\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "print(get_acc(\"baseline\"))\n",
    "print(get_acc(\"random\"))\n",
    "print(get_acc(\"prune\"))\n",
    "print(get_acc(\"baseline\").values())\n",
    "print(get_acc(\"random\").values())\n",
    "print(get_acc(\"prune\").values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2526302/2015199869.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  logits = torch.load(f\"{model_dir}/baseline.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_experts shape torch.Size([16, 5350069, 8])\n",
      "expert_frequency shape torch.Size([16, 64])\n",
      "expert_frequency tensor([[1840042,  243950,  254206,  ...,  471582,  286382,  994916],\n",
      "        [ 465830, 1924910,  405280,  ...,  378917,  684210,  673798],\n",
      "        [ 481770, 1703642,  180686,  ...,  361239,  899755,  882962],\n",
      "        ...,\n",
      "        [ 483036,  644833,  526679,  ...,  189351,  229874,  562050],\n",
      "        [ 425265,  813838, 1826416,  ...,  418472,  592490,  788973],\n",
      "        [1523279,  381101,  724180,  ...,  641075,  451530, 1600010]],\n",
      "       dtype=torch.int32)\n",
      "entropy tensor([5.6433, 5.7174, 5.7027, 5.6944, 5.6423, 5.7080, 5.6782, 5.7117, 5.7021,\n",
      "        5.7218, 5.6797, 5.6942, 5.7504, 5.7029, 5.7318, 5.7342])\n",
      "average entropy tensor(5.7009)\n",
      "max entropy tensor(5.7504)\n",
      "entropy for each layer [5.643259048461914, 5.7174072265625, 5.70271635055542, 5.694390773773193, 5.642325401306152, 5.707958221435547, 5.678200721740723, 5.711667060852051, 5.702147006988525, 5.721807956695557, 5.679737091064453, 5.694178581237793, 5.750357627868652, 5.702865123748779, 5.73181676864624, 5.7342000007629395]\n"
     ]
    }
   ],
   "source": [
    "def calculate_entropy(A):\n",
    "    A = A.float()\n",
    "    row_sums = A.sum(dim=-1, keepdim=True)\n",
    "    P = A / (row_sums + 1e-10)\n",
    "    P_log_P = P * torch.log2(P + 1e-10)\n",
    "    entropy = -P_log_P.sum(dim=-1)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "logits = torch.load(f\"{model_dir}/baseline.pt\")\n",
    "selected_experts = torch.topk(logits, k=8, dim=-1).indices\n",
    "print(\"selected_experts shape\", selected_experts.shape)\n",
    "expert_frequency = torch.zeros((selected_experts.shape[0], 64), dtype=torch.int32)\n",
    "for i in range(selected_experts.shape[0]):\n",
    "    counts = torch.bincount(selected_experts[i].flatten(), minlength=64)\n",
    "    expert_frequency[i] = counts\n",
    "print(\"expert_frequency shape\", expert_frequency.shape)\n",
    "print(\"expert_frequency\", expert_frequency)\n",
    "\n",
    "entropy = calculate_entropy(expert_frequency)\n",
    "print(\"entropy\", entropy)\n",
    "print(\"average entropy\", entropy.mean())\n",
    "print(\"max entropy\", entropy.max())\n",
    "print(\"entropy for each layer\", entropy.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea6b4c2f15c45bdbdde855e14f37b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3219f1ebf67e4d67ba540ec16bc94885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[    450,    1138,    1348,  ...,    3310,    1228,     591],\n",
      "         [   5699,    5376,   21069,  ...,    7498,    3457,    3901],\n",
      "         [  29121,    1416,    5035,  ...,    2234,    2182,    2828],\n",
      "         ...,\n",
      "         [   4881,    6642,    6032,  ...,    1757,    2657,    2101],\n",
      "         [   6533,    2860,    2800,  ...,    1519,   15304,    8429],\n",
      "         [     79,     194,     108,  ...,    4712,     819,     264]],\n",
      "\n",
      "        [[  61109,   20409,   39898,  ...,   10334,    8005,   17453],\n",
      "         [  51033,   22263,   38673,  ...,    2389,    6445,   14033],\n",
      "         [   9119,    6520,    1738,  ...,    1807,     617,    1109],\n",
      "         ...,\n",
      "         [   2226,     993,    1673,  ...,   15056,   19325,   19356],\n",
      "         [    133,    2158,     473,  ...,     824,    1563,   13970],\n",
      "         [   1000,    4656,    1857,  ...,   36023,    7843,   26169]],\n",
      "\n",
      "        [[   3264,   11524,   12216,  ...,   13582,   99856,    4517],\n",
      "         [   1336,     396,    3134,  ...,    1476,    3287,     250],\n",
      "         [      9,       1,      57,  ...,     715,     114,      49],\n",
      "         ...,\n",
      "         [   1075,      77,     571,  ...,    8125,    8671,     175],\n",
      "         [  17324,   26135,     764,  ...,    1920,    2090,    2565],\n",
      "         [    797,     186,   10365,  ...,   41191,    1188,   54304]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    524,     196,    2019,  ...,     451,     202,    3727],\n",
      "         [     52,      36,     200,  ...,      49,       0,     415],\n",
      "         [    192,    1450,    1701,  ...,    3575,   10683,      16],\n",
      "         ...,\n",
      "         [   1118,   10233,    3933,  ...,    4674,     301,   14227],\n",
      "         [     43,       1,     700,  ...,       3,      48,       5],\n",
      "         [    972,   14279,      15,  ...,    2859,    2374,      27]],\n",
      "\n",
      "        [[      9,     363,      54,  ...,      88,     112,      82],\n",
      "         [    329,    2627,    3330,  ...,   22221,    4960,    8552],\n",
      "         [    534,     141,    1518,  ...,      79,     438,     243],\n",
      "         ...,\n",
      "         [    264,   12192,     241,  ...,    4353,     491,     670],\n",
      "         [    595,    1161,    1775,  ...,    1961,     376,    2535],\n",
      "         [   2315,   24548,    1449,  ...,    5431,   12200,    6002]],\n",
      "\n",
      "        [[      9,     157,     122,  ...,      12,      44,     353],\n",
      "         [    141,      52,  211363,  ...,     582,     123,       4],\n",
      "         [    690,      13,   10371,  ...,     197,   22288,     644],\n",
      "         ...,\n",
      "         [    165,      72,     300,  ...,    8619,   10278,    2351],\n",
      "         [     81,      19,    3199,  ...,      25,    1249,   40066],\n",
      "         [1240013,       5,    1417,  ...,      15,    3488,   13866]]],\n",
      "       dtype=torch.int32)\n",
      "entropy tensor([[2.9408, 5.2575, 4.9941,  ..., 5.1561, 5.2204, 4.7453],\n",
      "        [5.3820, 5.5326, 5.4805,  ..., 5.1508, 5.1311, 5.0281],\n",
      "        [5.1393, 5.4592, 4.7975,  ..., 4.9378, 5.2947, 5.0371],\n",
      "        ...,\n",
      "        [4.6654, 4.8886, 4.2231,  ..., 4.8403, 4.7300, 5.1640],\n",
      "        [4.2928, 5.4134, 4.4346,  ..., 4.0084, 4.8594, 4.7360],\n",
      "        [3.8173, 3.6491, 4.9683,  ..., 4.3769, 4.3146, 3.1531]])\n",
      "average entropy tensor(4.7694)\n",
      "shape of entropy torch.Size([16, 64])\n",
      "collaboration for each layer tensor([4.9405, 5.2817, 5.1493, 4.8129, 4.9403, 4.8633, 4.7902, 4.7777, 4.5626,\n",
      "        4.5040, 4.4294, 4.5979, 4.6678, 4.7328, 4.6689, 4.5913])\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "top_expert_frequency = []\n",
    "for layer in tqdm(range(logits.shape[0]), desc=\"layer\"):\n",
    "    top1_experts = torch.argmax(logits[layer], dim=1)  # 形状为 [2693448]\n",
    "\n",
    "    expert_logits = []\n",
    "    for expert_idx in tqdm(range(logits.shape[2]), desc=\"expert\", disable=True):\n",
    "        # 创建布尔掩码，标记出当前专家为 Top-1 的位置\n",
    "        mask = top1_experts == expert_idx  # 形状为 [2,693,448]\n",
    "        # 使用掩码筛选出对应的 logits\n",
    "        selected_logits = logits[layer, mask]\n",
    "        expert_logits.append(selected_logits)\n",
    "        # print(\"selected_logits shape\", selected_logits.shape)\n",
    "    top_expert_frequency.append(expert_logits)\n",
    "\n",
    "collaboration = torch.zeros(\n",
    "    (logits.shape[0], logits.shape[2], logits.shape[2] - 1), dtype=torch.int32\n",
    ")\n",
    "for layer in tqdm(range(logits.shape[0]), desc=\"layer\"):\n",
    "    for expert in tqdm(range(logits.shape[2]), desc=\"expert\", disable=True):\n",
    "        expert_logits = top_expert_frequency[layer][expert]\n",
    "        expert_logits[:, expert] = float(\"-inf\")\n",
    "        top_experts = torch.topk(expert_logits, k=7, dim=-1).indices\n",
    "        collaboration_frequency = torch.bincount(top_experts.flatten(), minlength=64)\n",
    "        mask = torch.arange(64) != expert\n",
    "        collaboration[layer, expert] = collaboration_frequency[mask]\n",
    "\n",
    "entropy = calculate_entropy(collaboration)\n",
    "print(collaboration)\n",
    "print(\"entropy\", entropy)\n",
    "print(\"average entropy\", entropy.mean())\n",
    "print(\"shape of entropy\", entropy.shape)\n",
    "print(\"collaboration for each layer\", entropy.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-CoNa.json: average harmfulness 2.790157190367077\n",
      "I-CoNa.json: average llamaguard 0.7528089887640449\n",
      "I-CoNa.json: average openai 0.5115669894124469\n",
      "2.790157190367077, 0.7528089887640449, 0.5115669894124469\n",
      "I-Controversial.json: average harmfulness 2.6914047837257384\n",
      "I-Controversial.json: average llamaguard 0.7\n",
      "I-Controversial.json: average openai 0.329106279740407\n",
      "2.6914047837257384, 0.7, 0.329106279740407\n",
      "I-MaliciousInstructions.json: average harmfulness 2.4393502897769213\n",
      "I-MaliciousInstructions.json: average llamaguard 0.54\n",
      "I-MaliciousInstructions.json: average openai 0.23379952376668833\n",
      "2.4393502897769213, 0.54, 0.23379952376668833\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"I-CoNa.json\", \"I-Controversial.json\", \"I-MaliciousInstructions.json\"]\n",
    "for dataset in datasets:\n",
    "    result_path = f\"output/{os.path.basename(model_dir)}/{dataset}\"\n",
    "    result = json.load(open(result_path))\n",
    "    harmfulness = [item[\"harmfulness\"] for item in result]\n",
    "    llamaguard = [item[\"llamaguard\"] != \"safe\" for item in result]\n",
    "    openai = [item[\"opanai\"] for item in result]\n",
    "    print(f\"{dataset}: average harmfulness {sum(harmfulness) / len(harmfulness)}\")\n",
    "    print(f\"{dataset}: average llamaguard {sum(llamaguard) / len(llamaguard)}\")\n",
    "    print(f\"{dataset}: average openai {sum(openai) / len(openai)}\")\n",
    "    print(\n",
    "        f\"{sum(harmfulness) / len(harmfulness)}, {sum(llamaguard) / len(llamaguard)}, {sum(openai) / len(openai)}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
